{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import networkx as nx\n",
        "from typing import Any, Dict, Hashable, List, Tuple, Callable\n",
        "from collections import defaultdict\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from typing import Union, List, Tuple, Dict, Any, Hashable, Callable\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from node2vec import Node2Vec\n",
        "from community import community_louvain  # python-louvain package\n",
        "from collections import defaultdict\n",
        "from itertools import combinations\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "def l2_distance(vec1, vec2):\n",
        "    \"\"\"\n",
        "    Computes the L2 norm (Euclidean distance) between two 128-dimensional vectors.\n",
        "\n",
        "    Parameters:\n",
        "        vec1 (np.ndarray): First vector, shape (128,)\n",
        "        vec2 (np.ndarray): Second vector, shape (128,)\n",
        "\n",
        "    Returns:\n",
        "        float: L2 distance between vec1 and vec2\n",
        "    \"\"\"\n",
        "    return np.linalg.norm(vec1 - vec2)\n",
        "\n",
        "\n",
        "def a_star_search(graph: nx.Graph, source: Any, destination: Any, heuristic: Callable[[Any, Any], float] = l2_distance):\n",
        "    \"\"\"\n",
        "    Runs the A* algorithm on a NetworkX graph with a 2-arg heuristic.\n",
        "\n",
        "    Parameters:\n",
        "    - graph: A NetworkX graph.\n",
        "    - source: The starting node.\n",
        "    - destination: The goal node.\n",
        "    - heuristic: A function h(n, goal) that estimates the cost from node n to the goal.\n",
        "\n",
        "    Returns:\n",
        "    - path: List of nodes representing the shortest path.\n",
        "    - cost: Total cost of the path.\n",
        "    \"\"\"\n",
        "    def heuristic_wrapper(n1, n2):\n",
        "        # Use destination from outer scope, ignore n2 parameter\n",
        "        return heuristic(n1, destination)\n",
        "\n",
        "    try:\n",
        "        path = nx.astar_path(graph, source, destination, heuristic=heuristic_wrapper)\n",
        "        cost = nx.path_weight(graph, path, weight='weight')\n",
        "        return path, cost\n",
        "    except nx.NetworkXNoPath:\n",
        "        return None, float('inf')\n",
        "\n",
        "\n",
        "class EmbeddedCommunity:\n",
        "    def __init__(self, graph: nx.Graph, bounds):\n",
        "        \"\"\"\n",
        "        Initializes the EmbeddedGraph with a NetworkX graph.\n",
        "\n",
        "        Args:\n",
        "            graph (nx.Graph): Input graph to embed and decompose into communities.\n",
        "            bounds: Boundary nodes for this community\n",
        "        \"\"\"\n",
        "        self.graph = graph.copy()  # Create a copy to avoid modifying the original\n",
        "        self.community_bounds = bounds\n",
        "        self.node_embeddings = {}\n",
        "\n",
        "    def _enumerate_paths(self,source):\n",
        "      distance_to_bounds = []\n",
        "      for b in self.community_bounds:\n",
        "        distance_to_bounds.append(l2_distance(self.node_embeddings[source],self.node_embeddings[b]))\n",
        "\n",
        "      return distance_to_bounds\n",
        "\n",
        "\n",
        "\n",
        "    def embed(self) -> Dict[Hashable, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Runs node2vec embedding on the graph and creates an embedding for each node.\n",
        "\n",
        "        Returns:\n",
        "            Dict[Hashable, np.ndarray]: Mapping from node ID to embedding vector.\n",
        "        \"\"\"\n",
        "        embedded_nodes = defaultdict(int)\n",
        "        embedding_dim = 128  # Default embedding dimension\n",
        "\n",
        "        # Extract the subgraph for this community\n",
        "        subgraph = self.graph\n",
        "\n",
        "        # Skip communities that are too small for meaningful embedding\n",
        "        if len(subgraph.nodes) < 2:\n",
        "            # For isolated nodes, create a random embedding\n",
        "            for node in subgraph.nodes():\n",
        "                embedding = np.random.normal(0, 0.1, embedding_dim)\n",
        "                embedded_nodes[node] = embedding\n",
        "            return embedded_nodes\n",
        "\n",
        "        try:\n",
        "            # Configure node2vec\n",
        "            node2vec = Node2Vec(\n",
        "                subgraph,\n",
        "                dimensions=embedding_dim,\n",
        "                walk_length=30,\n",
        "                num_walks=200,\n",
        "                workers=4,\n",
        "                p=1,  # Return parameter\n",
        "                q=1,  # In-out parameter\n",
        "            )\n",
        "\n",
        "            # Train the model\n",
        "            model = node2vec.fit(window=10, min_count=1)\n",
        "\n",
        "            # Extract embeddings for each node\n",
        "            for node in subgraph.nodes():\n",
        "                try:\n",
        "                    # Convert node to string for node2vec lookup\n",
        "                    node_str = str(node)\n",
        "                    embedding = np.array(model.wv[node_str])\n",
        "                    embedded_nodes[node] = embedding\n",
        "                except KeyError:\n",
        "                    # Handle nodes that might not have been embedded properly\n",
        "                    embedding = np.random.normal(0, 0.1, embedding_dim)\n",
        "                    embedded_nodes[node] = embedding\n",
        "        except Exception as e:\n",
        "            # Fallback: Create random embeddings if node2vec fails\n",
        "            for node in subgraph.nodes():\n",
        "                embedding = np.random.normal(0, 0.1, embedding_dim)\n",
        "                embedded_nodes[node] = embedding\n",
        "\n",
        "        self.node_embeddings = embedded_nodes\n",
        "        return embedded_nodes\n",
        "\n",
        "    def get_top_k_boundary_nodes(self, k=5) -> List[Hashable]:\n",
        "        \"\"\"\n",
        "        Selects top-k boundary nodes using degree, PageRank, and L2 to mean embedding.\n",
        "\n",
        "        Returns:\n",
        "            List[Hashable]: Top-k nodes.\n",
        "        \"\"\"\n",
        "        candidates = set(self.community_bounds)\n",
        "\n",
        "        if not candidates:\n",
        "            return []\n",
        "\n",
        "        scores = {}\n",
        "        mean_embedding = np.mean(\n",
        "            [self.node_embeddings[n] for n in self.graph.nodes if n in self.node_embeddings], axis=0\n",
        "        )\n",
        "\n",
        "        pagerank = nx.pagerank(self.graph)\n",
        "\n",
        "        for node in candidates:\n",
        "            if node not in self.node_embeddings:\n",
        "                continue\n",
        "            deg = self.graph.degree(node)\n",
        "            pr = pagerank.get(node, 0)\n",
        "            l2 = -np.linalg.norm(self.node_embeddings[node] - mean_embedding)  # closer = higher score\n",
        "            scores[node] = deg + pr * 100 + l2  # weighted sum\n",
        "\n",
        "        sorted_nodes = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "        return [node for node, _ in sorted_nodes[:k]]\n",
        "\n",
        "\n",
        "\n",
        "class EmbeddedGraph:\n",
        "    def __init__(self, graph: nx.Graph):\n",
        "        \"\"\"\n",
        "        Initializes the EmbeddedGraph with a NetworkX graph.\n",
        "\n",
        "        Args:\n",
        "            graph (nx.Graph): Input graph to embed and decompose into communities.\n",
        "        \"\"\"\n",
        "        self.graph = graph.copy()  # Create a copy to avoid modifying the original\n",
        "        self.community_to_nodes = {}\n",
        "        self.node_to_community = {}\n",
        "        self.node_embeddings = defaultdict(int)\n",
        "        self.community_bounds = defaultdict(list)\n",
        "        self.embedded_communities = {}\n",
        "        self.community_graph = nx.Graph()\n",
        "\n",
        "        self._run_pipeline()\n",
        "\n",
        "    def _run_pipeline(self) -> None:\n",
        "        \"\"\"\n",
        "        Internal method that runs the full embedding pipeline:\n",
        "        Louvain → per-community node2vec → inter-edge analysis → community graph construction.\n",
        "        \"\"\"\n",
        "        # Step 1: Detect communities using Louvain method\n",
        "        self.community_to_nodes = self.run_louvain()\n",
        "\n",
        "        # Build a lookup of node -> community_id\n",
        "        for community_id, nodes in self.community_to_nodes.items():\n",
        "            for node in nodes:\n",
        "                self.node_to_community[node] = community_id\n",
        "\n",
        "        # Find edges between different communities\n",
        "        for u, v in self.graph.edges():\n",
        "            if self.node_to_community.get(u) != self.node_to_community.get(v):\n",
        "                self.community_bounds[self.node_to_community.get(u)].append(u)\n",
        "                self.community_bounds[self.node_to_community.get(v)].append(v)\n",
        "\n",
        "        # Step 2: Embed nodes within each community\n",
        "        # Process each community separately\n",
        "        for community_id, nodes in self.community_to_nodes.items():\n",
        "            # Extract the subgraph for this community\n",
        "            subgraph = self.graph.subgraph(nodes)\n",
        "            # Skip communities that are too small for meaningful embedding\n",
        "            if len(subgraph.nodes) < 2:\n",
        "                continue\n",
        "            embedded_community = EmbeddedCommunity(subgraph, self.community_bounds[community_id])\n",
        "            embedded_community.embed()\n",
        "\n",
        "            self.embedded_communities[community_id] = embedded_community\n",
        "            self.node_embeddings.update(embedded_community.node_embeddings)\n",
        "            #self.community_bounds[community_id] = embedded_community.get_top_k_boundary_nodes(k=5)\n",
        "\n",
        "\n",
        "        # Step 5: Build community-level graph\n",
        "        self.community_graph = self.construct_skeleton_graph()\n",
        "\n",
        "    def run_louvain(self) -> Dict[int, List[Hashable]]:\n",
        "        \"\"\"\n",
        "        Detects communities using the Louvain method.\n",
        "\n",
        "        Returns:\n",
        "            Dict[int, List[Hashable]]: Mapping from community ID to list of node IDs.\n",
        "        \"\"\"\n",
        "        # Run Louvain community detection\n",
        "        partition = community_louvain.best_partition(self.graph)\n",
        "\n",
        "        # Group nodes by community\n",
        "        communities = {}\n",
        "        for node, community_id in partition.items():\n",
        "            if community_id not in communities:\n",
        "                communities[community_id] = []\n",
        "            communities[community_id].append(node)\n",
        "\n",
        "        return communities\n",
        "\n",
        "    def construct_skeleton_graph(self):\n",
        "        \"\"\"\n",
        "        Constructs a skeleton graph connecting boundary nodes across communities.\n",
        "\n",
        "        Returns:\n",
        "            nx.Graph: The skeleton graph\n",
        "        \"\"\"\n",
        "        skeleton = nx.Graph()\n",
        "\n",
        "        # Step 1: Add intra-community boundary node connections\n",
        "        for community_id, boundary_nodes in self.community_bounds.items():\n",
        "            for u, v in combinations(boundary_nodes, 2):\n",
        "                if u in self.node_embeddings and v in self.node_embeddings:\n",
        "                    weight = np.linalg.norm(self.node_embeddings[u] - self.node_embeddings[v])\n",
        "                    skeleton.add_edge(u, v, weight=weight)\n",
        "\n",
        "        # Step 2: Add inter-community edges from original graph (only if the edge crosses communities)\n",
        "        for u, v in self.graph.edges():\n",
        "            cu = self.node_to_community.get(u)\n",
        "            cv = self.node_to_community.get(v)\n",
        "            if cu is not None and cv is not None and cu != cv:\n",
        "                skeleton.add_edge(u, v, weight=1)\n",
        "\n",
        "        return skeleton\n",
        "\n",
        "\n",
        "def l2_distance(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
        "    return np.linalg.norm(vec1 - vec2)\n",
        "\n",
        "\n",
        "def a_star_search(graph: nx.Graph, source: Any, destination: Any,\n",
        "                  heuristic: Callable[[Any, Any], float] = l2_distance) -> Tuple[list, float]:\n",
        "    def heuristic_wrapper(n1, _):\n",
        "        return heuristic(n1, destination)\n",
        "\n",
        "    try:\n",
        "        path = nx.astar_path(graph, source, destination, heuristic=heuristic_wrapper, weight='weight')\n",
        "        cost = nx.path_weight(graph, path, weight='weight')\n",
        "        return path, cost\n",
        "    except nx.NetworkXNoPath:\n",
        "        return None, float('inf')\n",
        "    except Exception as e:\n",
        "        print(f\"[A* ERROR] {e}\")\n",
        "        return None, float('inf')\n",
        "\n",
        "\n",
        "class HierarchicalEmbeddedGraph:\n",
        "    def __init__(self, graph: nx.Graph, max_nodes: int, num_levels: int):\n",
        "        self.original_graph = graph\n",
        "        self.num_levels = num_levels\n",
        "        self.max_nodes = max_nodes\n",
        "        self.embedded_graphs = []\n",
        "\n",
        "        self._build_hierarchy()\n",
        "\n",
        "        self.node_to_comm = defaultdict(dict)\n",
        "        self.bitmask = defaultdict(int)\n",
        "        self._build_indexing()\n",
        "        self.ensure_edge_weights()\n",
        "\n",
        "    def _build_hierarchy(self) -> None:\n",
        "        current_graph = self.original_graph\n",
        "\n",
        "        for level in range(self.num_levels):\n",
        "            embedded_graph = EmbeddedGraph(current_graph)\n",
        "            self.embedded_graphs.append(embedded_graph)\n",
        "\n",
        "            if len(embedded_graph.community_graph.nodes) <= 5:\n",
        "                break\n",
        "\n",
        "            current_graph = embedded_graph.community_graph\n",
        "\n",
        "        if self.embedded_graphs:\n",
        "            first_level = self.embedded_graphs[0]\n",
        "            self.graph = first_level.graph\n",
        "            self.community_to_nodes = first_level.community_to_nodes\n",
        "            self.node_embeddings = first_level.node_embeddings\n",
        "            self.community_graph = first_level.community_graph\n",
        "\n",
        "    def _build_indexing(self):\n",
        "        for level, embedded_graph in enumerate(self.embedded_graphs):\n",
        "            for node, comm_id in embedded_graph.node_to_community.items():\n",
        "                self.node_to_comm[level][node] = comm_id\n",
        "                bit_position = hash(comm_id) % 64\n",
        "                self.bitmask[node] |= (1 << bit_position)\n",
        "\n",
        "    def shared_level(self, u, v):\n",
        "        if self.bitmask[u] & self.bitmask[v] == 0:\n",
        "            return None\n",
        "\n",
        "        for l in range(len(self.embedded_graphs)):\n",
        "            if self.node_to_comm[l].get(u) == self.node_to_comm[l].get(v):\n",
        "                return l\n",
        "\n",
        "        return None\n",
        "\n",
        "    def ensure_edge_weights(self, default_weight: float = 1.0, verbose: bool = True):\n",
        "        for level, embedded in enumerate(self.embedded_graphs):\n",
        "            graph = embedded.graph\n",
        "            patched = 0\n",
        "            for u, v in graph.edges():\n",
        "                if 'weight' not in graph[u][v]:\n",
        "                    graph[u][v]['weight'] = default_weight\n",
        "                    patched += 1\n",
        "            if verbose:\n",
        "                print(f\"[LEVEL {level}] Patched {patched} edges in `graph`.\")\n",
        "\n",
        "            community_graph = embedded.community_graph\n",
        "            patched = 0\n",
        "            for u, v in community_graph.edges():\n",
        "                if 'weight' not in community_graph[u][v]:\n",
        "                    community_graph[u][v]['weight'] = default_weight\n",
        "                    patched += 1\n",
        "            if verbose:\n",
        "                print(f\"[LEVEL {level}] Patched {patched} edges in `community_graph`.\")\n",
        "\n",
        "    def parallel_astar_to_boundaries(self, node, level=0, k=5):\n",
        "        graph = self.embedded_graphs[level].graph\n",
        "        embeddings = self.embedded_graphs[level].node_embeddings\n",
        "        node_to_comm = self.embedded_graphs[level].node_to_community\n",
        "        comm_id = node_to_comm.get(node)\n",
        "\n",
        "        if comm_id is None:\n",
        "            print(f\"[PARALLEL] Node {node} not found in community map at level {level}\")\n",
        "            return {}\n",
        "\n",
        "        boundaries = self.embedded_graphs[level].community_bounds.get(comm_id, [])[:k]\n",
        "        results = {}\n",
        "\n",
        "        def search_to_target(target):\n",
        "            if node not in embeddings or target not in embeddings:\n",
        "                print(f\"[PARALLEL] Missing embeddings for node {node} or target {target}\")\n",
        "                return target, (None, float('inf'))\n",
        "\n",
        "            def heuristic(u, _):\n",
        "                return l2_distance(embeddings[u], embeddings[target])\n",
        "\n",
        "            return target, a_star_search(graph, node, target, heuristic=heuristic)\n",
        "\n",
        "        with ThreadPoolExecutor() as executor:\n",
        "            future_to_target = {executor.submit(search_to_target, t): t for t in boundaries}\n",
        "            for future in as_completed(future_to_target):\n",
        "                target, result = future.result()\n",
        "                results[target] = result\n",
        "                print(f\"[PARALLEL] {node} → {target}: {result}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def sequential_astar_to_boundaries(self, node, level=0, k=5):\n",
        "        graph = self.embedded_graphs[level].graph\n",
        "        embeddings = self.embedded_graphs[level].node_embeddings\n",
        "        node_to_comm = self.embedded_graphs[level].node_to_community\n",
        "        comm_id = node_to_comm.get(node)\n",
        "\n",
        "        if comm_id is None:\n",
        "            print(f\"[SEQUENTIAL] Node {node} not found in community map at level {level}\")\n",
        "            return {}\n",
        "\n",
        "        boundaries = self.embedded_graphs[level].community_bounds.get(comm_id, [])[:k]\n",
        "        results = {}\n",
        "\n",
        "        for target in boundaries:\n",
        "            if node not in embeddings or target not in embeddings:\n",
        "                print(f\"[SEQUENTIAL] Missing embeddings for node {node} or target {target}\")\n",
        "                results[target] = (None, float('inf'))\n",
        "                continue\n",
        "\n",
        "            def heuristic(u, _):\n",
        "                return l2_distance(embeddings[u], embeddings[target])\n",
        "\n",
        "            path, cost = a_star_search(graph, node, target, heuristic=heuristic)\n",
        "            results[target] = (path, cost)\n",
        "            print(f\"[SEQUENTIAL] {node} → {target}: {path}, cost={cost}\")\n",
        "\n",
        "        return results\n"
      ],
      "metadata": {
        "id": "pCQpcrIusiQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now that the full implementation is defined, we can run the benchmark using the 'g' graph\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from typing import List, Tuple\n",
        "\n",
        "def generate_random_node_pairs(graph: nx.Graph, num_pairs: int = 10) -> List[Tuple[int, int]]:\n",
        "    nodes = list(graph.nodes())\n",
        "    pairs = []\n",
        "    while len(pairs) < num_pairs:\n",
        "        src, dst = np.random.choice(nodes, 2, replace=False)\n",
        "        if nx.has_path(graph, src, dst):\n",
        "            pairs.append((src, dst))\n",
        "    return pairs\n",
        "\n",
        "def benchmark_astar_vs_networkx(h_graph, graph: nx.Graph, node_pairs: List[Tuple[int, int]],\n",
        "                                 level=0, k=5, min_length=3):\n",
        "    parallel_total_time = 0.0\n",
        "    networkx_total_time = 0.0\n",
        "    parallel_calls = 0\n",
        "    dijkstra_calls = 0\n",
        "\n",
        "    for src, _ in node_pairs:\n",
        "        comm_map = h_graph.embedded_graphs[level].node_to_community\n",
        "        comm_id = comm_map.get(src)\n",
        "        if comm_id is None:\n",
        "            continue\n",
        "\n",
        "        boundaries = h_graph.embedded_graphs[level].community_bounds.get(comm_id, [])\n",
        "        filtered_targets = []\n",
        "\n",
        "        for target in boundaries:\n",
        "            if src == target:\n",
        "                continue\n",
        "            try:\n",
        "                path = nx.shortest_path(graph, src, target)\n",
        "                if len(path) - 1 >= min_length:\n",
        "                    filtered_targets.append(target)\n",
        "            except nx.NetworkXNoPath:\n",
        "                continue\n",
        "\n",
        "            if len(filtered_targets) >= k:\n",
        "                break\n",
        "\n",
        "        if len(filtered_targets) < k:\n",
        "            continue\n",
        "\n",
        "        h_graph.embedded_graphs[level].community_bounds[comm_id] = filtered_targets\n",
        "\n",
        "        # Parallel A*\n",
        "        start = time.time()\n",
        "        _ = h_graph.parallel_astar_to_boundaries(src, level=level, k=k)\n",
        "        end = time.time()\n",
        "        parallel_total_time += (end - start)\n",
        "        parallel_calls += k\n",
        "\n",
        "        # NetworkX Dijkstra for each target\n",
        "        for target in filtered_targets:\n",
        "            start = time.time()\n",
        "            try:\n",
        "                _ = nx.dijkstra_path(graph, src, target)\n",
        "                networkx_total_time += time.time() - start\n",
        "                dijkstra_calls += 1\n",
        "            except nx.NetworkXNoPath:\n",
        "                continue\n",
        "\n",
        "    return parallel_total_time / max(parallel_calls, 1), networkx_total_time / max(dijkstra_calls, 1)\n",
        "\n",
        "\n",
        "\n",
        "def summarize_results(parallel_times, networkx_times):\n",
        "    def summarize(name, times):\n",
        "        print(f\"{name} AVERAGE TIME: {np.mean(times):.6f}s over {len(times)} runs\")\n",
        "    summarize(\"Parallel A*\", parallel_times)\n",
        "    summarize(\"NetworkX Dijkstra\", networkx_times)\n",
        "\n",
        "# Assuming 'g' is preloaded Citeseer graph\n",
        "#h_graph = HierarchicalEmbeddedGraph(g, max_nodes=500, num_levels=2)\n",
        "#h_graph.ensure_edge_weights()\n",
        "h_graph = heg\n",
        "g = G\n",
        "#pairs = generate_random_node_pairs(G, num_pairs=100)\n",
        "pairs = generate_random_node_pairs(g, num_pairs=100)\n",
        "p_avg, nx_avg = benchmark_astar_vs_networkx(h_graph, g, pairs, level=0, k=10, min_length=5)\n",
        "\n",
        "print(f\"Parallel A* Avg Time per Path: {p_avg:.6f}s\")\n",
        "print(f\"NetworkX Dijkstra Avg Time per Path: {nx_avg:.6f}s\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDnCUFLL2HFZ",
        "outputId": "0e7aabc3-9755-4a56-b857-4aad5d643a16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PARALLEL] 1022 → 1033: ([1022, 1461, 2918, 1710, 2112, 1033], 5.0)\n",
            "[PARALLEL] 1022 → 2816: ([1022, 1461, 2918, 1710, 2112, 1033, 411, 1410, 1324, 2816], 9.0)\n",
            "[PARALLEL] 1022 → 411: ([1022, 1461, 2918, 1710, 2112, 1033, 411], 6.0)\n",
            "[PARALLEL] 1022 → 430: ([1022, 1461, 2918, 1710, 2112, 1033, 2819, 1992, 455, 1684, 430], 10.0)\n",
            "[PARALLEL] 1022 → 455: ([1022, 1461, 2918, 1710, 2112, 1033, 2819, 1992, 455], 8.0)\n",
            "[PARALLEL] 1022 → 1865: ([1022, 1461, 2215, 1710, 2112, 1307, 1865], 6.0)\n",
            "[PARALLEL] 1022 → 2973: ([1022, 1461, 2918, 1710, 2112, 1033, 411, 1410, 2345, 465, 2973], 10.0)\n",
            "[PARALLEL] 1022 → 1992: ([1022, 1461, 2918, 1710, 2112, 1033, 2819, 1992], 7.0)\n",
            "[PARALLEL] 1022 → 1410: ([1022, 1461, 2918, 1710, 2112, 1033, 411, 1410], 7.0)\n",
            "[PARALLEL] 1022 → 2819: ([1022, 1461, 2918, 1710, 2112, 1033, 2819], 6.0)\n",
            "Parallel A* Avg Time per Path: 0.000914s\n",
            "NetworkX Dijkstra Avg Time per Path: 0.000341s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.utils import to_networkx\n",
        "\n",
        "from torch_geometric.datasets import Planetoid\n",
        "import torch_geometric.transforms as T\n",
        "\n",
        "# Load the CiteSeer dataset\n",
        "dataset = Planetoid(root='/tmp/CiteSeer', name='CiteSeer', transform=T.NormalizeFeatures())\n",
        "data = dataset[0]  # The graph object\n",
        "\n",
        "\n",
        "from torch_geometric.utils import to_networkx\n",
        "\n",
        "# Convert to undirected NetworkX graph\n",
        "G = to_networkx(data, to_undirected=True)\n",
        "\n",
        "print(f\"Graph has {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7TlZm1mdhh2",
        "outputId": "be86258a-2257-47c9-ba8d-e1514e23e8bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph has 3327 nodes and 4552 edges.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def debug_node_to_boundary(heg: HierarchicalEmbeddedGraph, node: Any, level: int = 0):\n",
        "    print(f\"\\n🔍 Debugging node {node} at level {level}...\")\n",
        "\n",
        "    graph = heg.embedded_graphs[level].graph\n",
        "    embeddings = heg.embedded_graphs[level].node_embeddings\n",
        "    node_to_comm = heg.embedded_graphs[level].node_to_community\n",
        "    comm_bounds = heg.embedded_graphs[level].community_bounds\n",
        "\n",
        "    # Check embedding\n",
        "    if node not in embeddings:\n",
        "        print(\"❌ Node has no embedding.\")\n",
        "    else:\n",
        "        print(\"✅ Node has an embedding.\")\n",
        "\n",
        "    # Check community\n",
        "    comm = node_to_comm.get(node)\n",
        "    if comm is None:\n",
        "        print(\"❌ Node has no community assignment.\")\n",
        "        return\n",
        "    else:\n",
        "        print(f\"✅ Node is in community {comm}\")\n",
        "\n",
        "    boundaries = comm_bounds.get(comm, [])\n",
        "    print(f\"📍 Top-k boundary nodes for this community: {boundaries}\")\n",
        "\n",
        "    for boundary in boundaries:\n",
        "        print(f\"\\n--- Checking path to boundary node {boundary} ---\")\n",
        "        if boundary not in embeddings:\n",
        "            print(\"⚠️  Boundary node has no embedding.\")\n",
        "        if not nx.has_path(graph, node, boundary):\n",
        "            print(\"🚫 No path in the graph.\")\n",
        "        else:\n",
        "            print(\"✅ Path exists in graph.\")\n",
        "            path = nx.shortest_path(graph, node, boundary)\n",
        "            print(f\"🧭 Path: {path}, Length: {len(path)-1}\")\n",
        "            dist = l2_distance(embeddings.get(node, np.zeros(128)), embeddings.get(boundary, np.zeros(128)))\n",
        "            print(f\"📐 Embedding L2 distance: {dist:.4f}\")\n",
        "debug_node_to_boundary(heg, 1424)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9o0dNd4hh8I",
        "outputId": "1f94ef14-2dc1-4948-de1f-c9f236b1c156"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔍 Debugging node 1424 at level 0...\n",
            "✅ Node has an embedding.\n",
            "✅ Node is in community 35\n",
            "📍 Top-k boundary nodes for this community: [2174, 2861, 2228, 49, 1253]\n",
            "\n",
            "--- Checking path to boundary node 2174 ---\n",
            "✅ Path exists in graph.\n",
            "🧭 Path: [1424, 709, 2174], Length: 2\n",
            "📐 Embedding L2 distance: 2.3412\n",
            "\n",
            "--- Checking path to boundary node 2861 ---\n",
            "✅ Path exists in graph.\n",
            "🧭 Path: [1424, 709, 1471, 2861], Length: 3\n",
            "📐 Embedding L2 distance: 2.5385\n",
            "\n",
            "--- Checking path to boundary node 2228 ---\n",
            "✅ Path exists in graph.\n",
            "🧭 Path: [1424, 709, 1471, 682, 2228], Length: 4\n",
            "📐 Embedding L2 distance: 2.9976\n",
            "\n",
            "--- Checking path to boundary node 49 ---\n",
            "✅ Path exists in graph.\n",
            "🧭 Path: [1424, 709, 1471, 682, 2228, 49], Length: 5\n",
            "📐 Embedding L2 distance: 3.0793\n",
            "\n",
            "--- Checking path to boundary node 1253 ---\n",
            "✅ Path exists in graph.\n",
            "🧭 Path: [1424, 709, 1471, 2861, 2455, 1253], Length: 5\n",
            "📐 Embedding L2 distance: 3.3356\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ogb\n",
        "!pip install node2vec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWVK2widdCcb",
        "outputId": "0505bd52-49ef-40ad-fe65-e9d1751880bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ogb\n",
            "  Downloading ogb-1.3.6-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (2.0.2)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (1.6.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (2.2.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (2.4.0)\n",
            "Collecting outdated>=0.2.0 (from ogb)\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.11/dist-packages (from outdated>=0.2.0->ogb) (75.2.0)\n",
            "Collecting littleutils (from outdated>=0.2.0->ogb)\n",
            "  Downloading littleutils-0.2.4-py3-none-any.whl.metadata (679 bytes)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from outdated>=0.2.0->ogb) (2.32.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.0->ogb) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.0->ogb) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.0->ogb) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->ogb) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->ogb) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->ogb) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6.0->ogb) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.6.0->ogb) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->outdated>=0.2.0->ogb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->outdated>=0.2.0->ogb) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->outdated>=0.2.0->ogb) (2025.4.26)\n",
            "Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading littleutils-0.2.4-py3-none-any.whl (8.1 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, littleutils, outdated, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ogb\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed littleutils-0.2.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ogb-1.3.6 outdated-0.2.2\n",
            "Collecting node2vec\n",
            "  Downloading node2vec-0.5.0-py3-none-any.whl.metadata (849 bytes)\n",
            "Collecting gensim<5.0.0,>=4.3.0 (from node2vec)\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: joblib<2.0.0,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from node2vec) (1.4.2)\n",
            "Requirement already satisfied: networkx<4.0.0,>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from node2vec) (3.4.2)\n",
            "Collecting numpy<2.0.0,>=1.24.0 (from node2vec)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from node2vec) (4.67.1)\n",
            "Collecting scipy<1.14.0,>=1.7.0 (from gensim<5.0.0,>=4.3.0->node2vec)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim<5.0.0,>=4.3.0->node2vec) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim<5.0.0,>=4.3.0->node2vec) (1.17.2)\n",
            "Downloading node2vec-0.5.0-py3-none-any.whl (7.2 kB)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim, node2vec\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.2\n",
            "    Uninstalling scipy-1.15.2:\n",
            "      Successfully uninstalled scipy-1.15.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 node2vec-0.5.0 numpy-1.26.4 scipy-1.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "\n",
        "from ogb.nodeproppred import PygNodePropPredDataset\n",
        "from torch_geometric.utils import to_networkx\n",
        "\n",
        "# === Load ogbn-arxiv Dataset and Build Graph ===\n",
        "dataset = PygNodePropPredDataset(name='ogbn-arxiv', root='data')\n",
        "data = dataset[0]\n",
        "G = to_networkx(data, to_undirected=True)\n",
        "\n",
        "# === Create Hierarchical Embedded Graph ===\n",
        "h_graph = HierarchicalEmbeddedGraph(G, max_nodes=500, num_levels=3)\n",
        "\n",
        "\n",
        "# === Sample Long-Distance Pairs ===\n",
        "def sample_distant_node_pairs(graph: nx.Graph, num_pairs: int = 20, min_distance: int = 5):\n",
        "    \"\"\"\n",
        "    Samples node pairs at least `min_distance` apart.\n",
        "    \"\"\"\n",
        "    nodes = list(graph.nodes)\n",
        "    pairs = []\n",
        "    attempts = 0\n",
        "    while len(pairs) < num_pairs and attempts < num_pairs * 10:\n",
        "        u, v = random.sample(nodes, 2)\n",
        "        try:\n",
        "            d = nx.shortest_path_length(graph, source=u, target=v)\n",
        "            if d >= min_distance:\n",
        "                pairs.append((u, v))\n",
        "        except nx.NetworkXNoPath:\n",
        "            pass\n",
        "        attempts += 1\n",
        "    return pairs\n",
        "\n",
        "\n",
        "# === Benchmark Latency Only ===\n",
        "def benchmark_latency_only(graph, h_graph, pairs):\n",
        "    results = []\n",
        "    for src, dst in pairs:\n",
        "        # Hierarchical\n",
        "        start = time.perf_counter()\n",
        "        h_path, h_dist = h_graph.find_hierarchical_path(src, dst)\n",
        "        h_time = time.perf_counter() - start\n",
        "\n",
        "        # NetworkX Dijkstra\n",
        "        try:\n",
        "            start = time.perf_counter()\n",
        "            nx_dist = nx.dijkstra_path_length(graph, src, dst)\n",
        "            nx_time = time.perf_counter() - start\n",
        "        except nx.NetworkXNoPath:\n",
        "            nx_dist = float('inf')\n",
        "            nx_time = time.perf_counter() - start\n",
        "\n",
        "        results.append((src, dst, h_time, nx_time, h_dist, nx_dist))\n",
        "    return results\n",
        "\n",
        "\n",
        "# === Print Benchmark Summary ===\n",
        "def print_latency_summary(results):\n",
        "    h_times = [r[2] for r in results]\n",
        "    nx_times = [r[3] for r in results]\n",
        "\n",
        "    print(\"\\n--- Latency Benchmark Summary ---\")\n",
        "    print(f\"Queries Run: {len(results)}\")\n",
        "    print(f\"Avg Hierarchical Query Time: {np.mean(h_times):.4f}s\")\n",
        "    print(f\"Avg NetworkX Dijkstra Time:  {np.mean(nx_times):.4f}s\")\n",
        "    print(\"\\nDetails per Query:\")\n",
        "    for r in results:\n",
        "        print(f\"Src: {r[0]}, Dst: {r[1]} | H: {r[2]:.4f}s | NX: {r[3]:.4f}s | H_dist: {r[4]:.2f} | NX_dist: {r[5]:.2f}\")\n",
        "\n",
        "\n",
        "# === Run Everything ===\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Sampling distant node pairs...\")\n",
        "    pairs = sample_distant_node_pairs(G, num_pairs=20, min_distance=6)\n",
        "\n",
        "    print(\"Running latency benchmark...\")\n",
        "    results = benchmark_latency_only(G, h_graph, pairs)\n",
        "\n",
        "    print_latency_summary(results)\n"
      ],
      "metadata": {
        "id": "VOnmWTmac7UN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7q-MGLH7OLQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "T1teJyuDbAW-",
        "outputId": "cdaaeb12-84f4-4b3d-ecfd-b8d7af7d99b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-34e62a6ea3c5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlanetoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Cora'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# or 'Citeseer', 'PubMed'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/datasets/planetoid.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, name, split, num_train_per_class, num_val, num_test, transform, pre_transform, force_reload)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'public'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'full'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'geom-gcn'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'random'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         super().__init__(root, transform, pre_transform,\n\u001b[0m\u001b[1;32m    103\u001b[0m                          force_reload=force_reload)\n\u001b[1;32m    104\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/data/in_memory_dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter, log, force_reload)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mforce_reload\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     ) -> None:\n\u001b[0;32m---> 81\u001b[0;31m         super().__init__(root, transform, pre_transform, pre_filter, log,\n\u001b[0m\u001b[1;32m     82\u001b[0m                          force_reload)\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/data/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter, log, force_reload)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_download\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_process\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/data/dataset.py\u001b[0m in \u001b[0;36m_download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/datasets/planetoid.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_file_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{self.url}/{name}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'geom-gcn'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/io/fs.py\u001b[0m in \u001b[0;36mcp\u001b[0;34m(path1, path2, extract, log, use_cache, clear_cache)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0mis_path1_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0mis_path2_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/io/fs.py\u001b[0m in \u001b[0;36misdir\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_fs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/io/fs.py\u001b[0m in \u001b[0;36mget_fs\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;34m:\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\"gs://home/me/file\"\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\"s3://...\"\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \"\"\"\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfsspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl_to_fs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fsspec/core.py\u001b[0m in \u001b[0;36murl_to_fs\u001b[0;34m(url, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m     }\n\u001b[1;32m    402\u001b[0m     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mknown_kwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m     \u001b[0mchain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_un_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m     \u001b[0minkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;31m# Reverse iterate the chain, creating a nested target_* structure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fsspec/core.py\u001b[0m in \u001b[0;36m_un_chain\u001b[0;34m(path, kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"protocol\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msplit_protocol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"file\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_filesystem_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m         \u001b[0mextra_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_kwargs_from_urls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0mkws\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fsspec/registry.py\u001b[0m in \u001b[0;36mget_filesystem_class\u001b[0;34m(protocol)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mbit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknown_implementations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m             \u001b[0mregister_implementation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_import_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"class\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"err\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fsspec/registry.py\u001b[0m in \u001b[0;36m_import_class\u001b[0;34m(fqp)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0mis_s3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"s3fs\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m     \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_s3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"5\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms3_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fsspec/implementations/http.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murlparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0maiohttp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0myarl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/aiohttp/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhdrs\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m from .client import (\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mBaseConnector\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mClientConnectionError\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/aiohttp/client.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0myarl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mURL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpayload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_websocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWebSocketDataQueue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mabc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAbstractCookieJar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/aiohttp/http.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mhttp_exceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHttpProcessingError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mHttpProcessingError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m from .http_parser import (\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mHeadersParser\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mHeadersParser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mHttpParser\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mHttpParser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/aiohttp/http_parser.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase_protocol\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseProtocol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcompression_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHAS_BROTLI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBrotliDecompressor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZLibDecompressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m from .helpers import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/aiohttp/base_protocol.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mclient_exceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClientConnectionResetError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mhelpers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtcp_helpers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtcp_nodelay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-geometric\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uj_WOVIkbysy",
        "outputId": "377046f8-fa2b-406d-9944-f1bf55887f53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.4.26)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Clean reinstall numpy and gensim to fix binary incompatibility\n",
        "!pip install --force-reinstall --no-cache-dir numpy\n",
        "!pip install --force-reinstall --no-cache-dir gensim\n",
        "!pip install --force-reinstall --no-cache-dir node2vec\n",
        "!pip install --force-reinstall --no-cache-dir python-louvain\n",
        "\n",
        "# Step 2: Restart runtime to clear out the C-extension issues\n",
        "import os\n",
        "os.kill(os.getpid(), 9)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wryWEyCaveMH",
        "outputId": "7c00ae34-e410-4032-cb3a-7fab892bd8b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n",
            "  Downloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/62.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m133.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/16.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/16.4 MB\u001b[0m \u001b[31m243.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m291.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.5\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m226.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m186.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smart-open>=1.8.1 (from gensim)\n",
            "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
            "  Downloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m183.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m175.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m214.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m178.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, numpy, smart-open, scipy, gensim\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.5\n",
            "    Uninstalling numpy-2.2.5:\n",
            "      Successfully uninstalled numpy-2.2.5\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 7.1.0\n",
            "    Uninstalling smart-open-7.1.0:\n",
            "      Successfully uninstalled smart-open-7.1.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1 smart-open-7.1.0 wrapt-1.17.2\n",
            "Collecting node2vec\n",
            "  Downloading node2vec-0.5.0-py3-none-any.whl.metadata (849 bytes)\n",
            "Collecting gensim<5.0.0,>=4.3.0 (from node2vec)\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting joblib<2.0.0,>=1.4.0 (from node2vec)\n",
            "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting networkx<4.0.0,>=3.1.0 (from node2vec)\n",
            "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting numpy<2.0.0,>=1.24.0 (from node2vec)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m225.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm<5.0.0,>=4.66.1 (from node2vec)\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m232.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim<5.0.0,>=4.3.0->node2vec)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m251.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smart-open>=1.8.1 (from gensim<5.0.0,>=4.3.0->node2vec)\n",
            "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting wrapt (from smart-open>=1.8.1->gensim<5.0.0,>=4.3.0->node2vec)\n",
            "  Downloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Downloading node2vec-0.5.0-py3-none-any.whl (7.2 kB)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m284.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m301.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m338.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m277.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m257.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m211.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m193.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m155.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tqdm, numpy, networkx, joblib, smart-open, scipy, gensim, node2vec\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.67.1\n",
            "    Uninstalling tqdm-4.67.1:\n",
            "      Successfully uninstalled tqdm-4.67.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.4.2\n",
            "    Uninstalling networkx-3.4.2:\n",
            "      Successfully uninstalled networkx-3.4.2\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.4.2\n",
            "    Uninstalling joblib-1.4.2:\n",
            "      Successfully uninstalled joblib-1.4.2\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 7.1.0\n",
            "    Uninstalling smart-open-7.1.0:\n",
            "      Successfully uninstalled smart-open-7.1.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 4.3.3\n",
            "    Uninstalling gensim-4.3.3:\n",
            "      Successfully uninstalled gensim-4.3.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 joblib-1.4.2 networkx-3.4.2 node2vec-0.5.0 numpy-1.26.4 scipy-1.13.1 smart-open-7.1.0 tqdm-4.67.1 wrapt-1.17.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "networkx"
                ]
              },
              "id": "00c7fe42c1c7417399108b447513d410"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-louvain\n",
            "  Downloading python-louvain-0.16.tar.gz (204 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/204.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.6/204.6 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting networkx (from python-louvain)\n",
            "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting numpy (from python-louvain)\n",
            "  Downloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m220.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m317.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m286.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    }
  ]
}